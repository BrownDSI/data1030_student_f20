{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set 5\n",
    "\n",
    "**Problem 0** (-2 points for every missing green OK sign. If you don't run the cell below, that's -14 points.)\n",
    "\n",
    "Make sure you are in the DATA1030 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.7.6 | packaged by conda-forge | (default, Jun  1 2020, 18:33:30) \n",
      "[Clang 9.0.1 ]\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 1.18.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.2.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 0.23.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 1.0.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m xgboost version 1.1.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m shap version 0.35.0 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from distutils.version import LooseVersion as Version\n",
    "import sys\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.7 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == min_ver:\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(sys.version)\n",
    "if pyversion >= \"3.7\":\n",
    "    print(OK, \"Python version is %s\" % sys.version)\n",
    "elif pyversion < \"3.7\":\n",
    "    print(FAIL, \"Python version 3.7 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % sys.version)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'numpy': \"1.18.5\", 'matplotlib': \"3.2.2\",'sklearn': \"0.23.1\", \n",
    "                'pandas': \"1.0.5\",'xgboost': \"1.1.1\", 'shap': \"0.35.0\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1** (14 points)\n",
    "\n",
    "You will implement a simple version of Little's test. The backbone of the function is provided below. Please work through and test each step as indicated in the comments and run the function on a couple of dummy datasets located in the `/data` folder.\n",
    "\n",
    "**NOTE1:** Step 1 is at the end of the cell outside of the `simple_Little_test` function. All other steps are inside the function.\n",
    "\n",
    "**NOTE2:** DO NOT USE THIS FUNCTION IN ANY SERIOUS PROJECT! This exercise is limited to the case when only one feature has missing values. If your dataset has missing values in multiple features, the function won't give accurate results. Please read [Little's paper](https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478722) if you want to implement the general approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature 1  feature 2  label\n",
      "0   1.308443        NaN      1\n",
      "1   0.579018   0.445624      0\n",
      "2   0.659703        NaN      0\n",
      "3   1.195591        NaN      0\n",
      "4   0.530409        NaN      0\n",
      "   feature 1  feature 2  feature 3  feature 4  label\n",
      "0  -0.053636   0.631832        NaN   0.830836      0\n",
      "1   0.707561  -1.265775   1.184195   0.268585      0\n",
      "2  -0.402403  -2.313096        NaN   0.051042      0\n",
      "3   1.348075  -1.366191  -0.172210   0.278011      0\n",
      "4   1.611499  -0.082858        NaN   0.216504      1\n",
      "   feature 1  feature 2  feature 3  feature 4  feature 5  feature 6  label\n",
      "0  -1.475149   0.529030   0.557537   3.819145        NaN  -0.959875      0\n",
      "1   0.072969  -0.397754  -0.980947   0.311793   0.646965   1.014624      0\n",
      "2   0.809812   1.036716   1.329798  -0.705942        NaN  -0.170632      1\n",
      "3  -0.067652   1.507020  -1.895661   0.977815   0.683140   0.760448      0\n",
      "4  -0.561700   0.589722   1.529195   1.182259        NaN   1.290034      0\n",
      "   feature 1  feature 2  feature 3  feature 4  feature 5  feature 6  \\\n",
      "0  -0.051216  -1.913776  -0.792629  -1.041014   2.093516  -2.435544   \n",
      "1  -3.121231   3.578809  -0.467891  -0.102485   0.576480   1.222446   \n",
      "2   1.080601  -0.038261  -0.618794  -0.429366  -0.479658  -0.142553   \n",
      "3   1.824874  -1.236415  -2.613018  -0.732713   0.107823   0.988119   \n",
      "4  -1.836256  -2.102403   3.197057   0.361456   1.088288   1.833173   \n",
      "\n",
      "   feature 7  feature 8  feature 9  feature 10  label  \n",
      "0   0.473848   0.675204   1.447888    0.120429      1  \n",
      "1   0.333764   0.215908   0.887364   -2.504906      0  \n",
      "2   1.343826  -0.093129   1.293488    1.328109      1  \n",
      "3        NaN   0.847495   0.692993    0.937383      0  \n",
      "4  -1.505481  -2.652183  -0.496211   -0.723545      1  \n"
     ]
    }
   ],
   "source": [
    "# first I generate the datasets they will test and import MCAR and MAR on one of the features\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# MAR\n",
    "n_samples = 300\n",
    "X, y = make_classification(n_samples=n_samples,n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=3, n_clusters_per_class=1,class_sep=0.5,weights=[0.7,0.3])\n",
    "\n",
    "xlim = [np.min(X[:, 0]),np.max(X[:, 0])]\n",
    "ylim = [np.min(X[:, 1]),np.max(X[:, 1])]\n",
    "\n",
    "f1_scaled = (MinMaxScaler().fit_transform(2**X[:, 0].reshape(-1, 1))[:,0])**2\n",
    "indcs = np.random.choice(np.arange(n_samples),size=int(n_samples*0.4),replace=False,p=f1_scaled/np.sum(f1_scaled))\n",
    "X_ampute = np.copy(X)\n",
    "X_ampute[indcs,1] = np.nan \n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['feature 1'] = X_ampute[:,0]\n",
    "df['feature 2'] = X_ampute[:,1]\n",
    "df['label'] = y\n",
    "df.to_csv('data/data1.csv',index=False)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# MCAR\n",
    "n_samples = 200\n",
    "X, y = make_classification(n_samples=n_samples,n_features=4, n_redundant=0, n_informative=2,\n",
    "                           random_state=10, n_clusters_per_class=1,class_sep=0.5)\n",
    "\n",
    "\n",
    "indcs = np.random.choice(np.arange(n_samples),size=int(n_samples/3),replace=False)\n",
    "X_ampute = np.copy(X)\n",
    "X_ampute[indcs,2] = np.nan\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['feature 1'] = X_ampute[:,0]\n",
    "df['feature 2'] = X_ampute[:,1]\n",
    "df['feature 3'] = X_ampute[:,2]\n",
    "df['feature 4'] = X_ampute[:,3]\n",
    "df['label'] = y\n",
    "df.to_csv('data/data2.csv',index=False)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# another MAR \n",
    "n_samples = 200\n",
    "X, y = make_classification(n_samples=n_samples,n_features=6, n_redundant=0, n_informative=3,\n",
    "                           random_state=3, n_clusters_per_class=1,class_sep=0.5,weights=[0.7,0.3])\n",
    "\n",
    "xlim = [np.min(X[:, 0]),np.max(X[:, 0])]\n",
    "ylim = [np.min(X[:, 1]),np.max(X[:, 1])]\n",
    "\n",
    "f1_scaled = (MinMaxScaler().fit_transform(2**np.abs(X[:, 0]).reshape(-1, 1))[:,0])**2\n",
    "indcs = np.random.choice(np.arange(n_samples),size=int(n_samples*0.4),replace=False,p=f1_scaled/np.sum(f1_scaled))\n",
    "X_ampute = np.copy(X)\n",
    "X_ampute[indcs,4] = np.nan \n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['feature 1'] = X_ampute[:,0]\n",
    "df['feature 2'] = X_ampute[:,1]\n",
    "df['feature 3'] = X_ampute[:,2]\n",
    "df['feature 4'] = X_ampute[:,3]\n",
    "df['feature 5'] = X_ampute[:,4]\n",
    "df['feature 6'] = X_ampute[:,5]\n",
    "df['label'] = y\n",
    "df.to_csv('data/data3.csv',index=False)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# last dataset with two features containing missing values\n",
    "n_samples = 1000\n",
    "X, y = make_classification(n_samples=n_samples,n_features=10, n_redundant=0, n_informative=6,\n",
    "                           random_state=10, n_clusters_per_class=1,class_sep=0.5)\n",
    "\n",
    "\n",
    "indcs = np.random.choice(np.arange(n_samples),size=int(n_samples/3),replace=False)\n",
    "indcs2 = np.random.choice(np.arange(n_samples),size=int(n_samples/3),replace=False)\n",
    "X_ampute = np.copy(X)\n",
    "X_ampute[indcs,1] = np.nan\n",
    "X_ampute[indcs2,6] = np.nan\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['feature 1'] = X_ampute[:,0]\n",
    "df['feature 2'] = X_ampute[:,1]\n",
    "df['feature 3'] = X_ampute[:,2]\n",
    "df['feature 4'] = X_ampute[:,3]\n",
    "df['feature 5'] = X_ampute[:,4]\n",
    "df['feature 6'] = X_ampute[:,5]\n",
    "df['feature 7'] = X_ampute[:,6]\n",
    "df['feature 8'] = X_ampute[:,7]\n",
    "df['feature 9'] = X_ampute[:,8]\n",
    "df['feature 10'] = X_ampute[:,9]\n",
    "df['label'] = y\n",
    "df.to_csv('data/data4.csv',index=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Multiple columns contain missing values, this function should not be used.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b7dfe0b81bd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/data'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_Little_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-b7dfe0b81bd4>\u001b[0m in \u001b[0;36msimple_Little_test\u001b[0;34m(df, p_crit)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#print(cols_with_nan)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols_with_nan\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Multiple columns contain missing values, this function should not be used.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# step 3: 2 points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Multiple columns contain missing values, this function should not be used."
     ]
    }
   ],
   "source": [
    "# feel free to import other packages if necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# remove this line:\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "def simple_Little_test(df, p_crit = 0.05):\n",
    "    # the input to the function is a pandas dataframe and a critical p_value which by default is set to 0.05.\n",
    "    \n",
    "    # this is the variable the function will return\n",
    "    # True if the missingness pattern correlates with any of the other columns (indicates missing at random)\n",
    "    # False if there is no correlation (it could be MCAR or MNAR)\n",
    "    # we set it to false for now so the function has a valid but potentially incorrect output\n",
    "    # we will update the value of the variable later\n",
    "    MAR_is_present = False\n",
    "    \n",
    "    # step 2: 3 points\n",
    "    # identify which columns(s) have the missing values\n",
    "    # if there are multiple columns with missing values, raise a ValueError \n",
    "    # and return a message saying 'Multiple columns contain missing values, this function should not be used.'\n",
    "    cols_with_nan = df.columns[df.isna().any()].tolist()\n",
    "    #print(cols_with_nan)\n",
    "    if len(cols_with_nan) > 1:\n",
    "        raise ValueError('Multiple columns contain missing values, this function should not be used.')\n",
    "    \n",
    "    # step 3: 2 points\n",
    "    # now that you identified the column with missing values, create a new column called 'mask' \n",
    "    # its value is 1 (or True) if an element of the column is NaN, its value is 0 (or False) otherwise.\n",
    "    df['mask'] = pd.isnull(df[cols_with_nan[0]])\n",
    "    #print(df.head())\n",
    "    \n",
    "    # step 4: 2 points\n",
    "    # collect the names of all columns that do not contain missing values and exclude the mask column\n",
    "    cols = list(df.columns)\n",
    "    cols.remove(cols_with_nan[0])\n",
    "    cols.remove('mask')\n",
    "    #print(cols)\n",
    "    \n",
    "    # step 5: 3 points\n",
    "    # decide which method to use to look for linear correlations between col and the mask.\n",
    "    # hint: assume that mask is your target variable.\n",
    "    # collect the p_values from the method\n",
    "    f, p_val = f_classif(df[cols],df['mask'])\n",
    "    #print(p_val)\n",
    "        \n",
    "        \n",
    "    # step 6: 2 points\n",
    "    # if any of the p_values are smaller than p_crit, MAR is present in the dataset\n",
    "    # change the value of MAR_is_present to true\n",
    "    # if all p_values are larger than critical, MAR_is_present can remain false.\n",
    "    if np.any(p_val < p_crit):\n",
    "        MAR_is_present = True\n",
    "    else:\n",
    "        MAR_is_present = False\n",
    "    \n",
    "    return MAR_is_present\n",
    "\n",
    "# step 1: 2 points\n",
    "# read in the four datasets in the `/data` folder and apply the function on each of them\n",
    "# print out what the function returns\n",
    "# initialally it will just return False four times\n",
    "\n",
    "for i in range(1,5):\n",
    "    df = pd.read_csv('data/data'+str(i)+'.csv')\n",
    "    print(simple_Little_test(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2** (16 points)\n",
    "\n",
    "Consider the [hand postures dataset](https://archive.ics.uci.edu/ml/datasets/Motion+Capture+Hand+Postures). You saw in the previous problem sets that it contains a large amount of missing values. Describe why you would or wouldn't use the techniques in the cells below to handle the missing values in this dataset. Feel free to use a mix of code and text answers to support your argument. The csv file is location in the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop the columns with missing values:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop the rows with missing values:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean or median imputation:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multivariate imputation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading suggestion:** 4 points per technique\n",
    "\n",
    "None of these techniques should be used with the hand postures dataset.\n",
    "\n",
    "Drop cols: one would need to drop all but the X1,Y1,Z1,X2,Y2,Z2 columns.\n",
    "\n",
    "Drop rows: one would need to drop 99.96% of the rows\n",
    "\n",
    "Mean and median imputation: always a bad idea\n",
    "\n",
    "Multivariate imputation: doesn't work here based on the description. \"The 11 markers not part of the rigid pattern were unlabeled; their positions were not explicitly tracked. Consequently, there is no a priori correspondence between the markers of two given records.\" So marker *i* for one user might be a different marker for a different user.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimensions: (78095, 38)\n",
      "fraction of missing values in features:\n",
      "X3     0.008835\n",
      "Y3     0.008835\n",
      "Z3     0.008835\n",
      "X4     0.039951\n",
      "Y4     0.039951\n",
      "Z4     0.039951\n",
      "X5     0.166758\n",
      "Y5     0.166758\n",
      "Z5     0.166758\n",
      "X6     0.330981\n",
      "Y6     0.330981\n",
      "Z6     0.330981\n",
      "X7     0.501338\n",
      "Y7     0.501338\n",
      "Z7     0.501338\n",
      "X8     0.608643\n",
      "Y8     0.608643\n",
      "Z8     0.608643\n",
      "X9     0.693105\n",
      "Y9     0.693105\n",
      "Z9     0.693105\n",
      "X10    0.811102\n",
      "Y10    0.811102\n",
      "Z10    0.811102\n",
      "X11    0.999603\n",
      "Y11    0.999603\n",
      "Z11    0.999603\n",
      "dtype: float64\n",
      "data types of the features with missing values:\n",
      "X3     object\n",
      "Y3     object\n",
      "Z3     object\n",
      "X4     object\n",
      "Y4     object\n",
      "Z4     object\n",
      "X5     object\n",
      "Y5     object\n",
      "Z5     object\n",
      "X6     object\n",
      "Y6     object\n",
      "Z6     object\n",
      "X7     object\n",
      "Y7     object\n",
      "Z7     object\n",
      "X8     object\n",
      "Y8     object\n",
      "Z8     object\n",
      "X9     object\n",
      "Y9     object\n",
      "Z9     object\n",
      "X10    object\n",
      "Y10    object\n",
      "Z10    object\n",
      "X11    object\n",
      "Y11    object\n",
      "Z11    object\n",
      "dtype: object\n",
      "fraction of points with missing values: 0.999603047570267\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/Postures.csv')\n",
    "df.drop([0],inplace=True)\n",
    "df.head()\n",
    "df.replace('?',np.nan,inplace=True)\n",
    "print('data dimensions:',df.shape)\n",
    "perc_missing_per_ftr = df.isnull().sum(axis=0)/df.shape[0]\n",
    "print('fraction of missing values in features:')\n",
    "print(perc_missing_per_ftr[perc_missing_per_ftr > 0])\n",
    "print('data types of the features with missing values:')\n",
    "print(df[perc_missing_per_ftr[perc_missing_per_ftr > 0].index].dtypes)\n",
    "frac_missing = sum(df.isnull().sum(axis=1)!=0)/df.shape[0]\n",
    "print('fraction of points with missing values:',frac_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
