{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set 4\n",
    "\n",
    "**Problem 0** (-2 points for every missing green OK sign. If you don't run the cell below, that's -14 points.)\n",
    "\n",
    "Make sure you are in the DATA1030 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.7.6 | packaged by conda-forge | (default, Jun  1 2020, 18:33:30) \n",
      "[Clang 9.0.1 ]\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 1.18.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.2.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 0.23.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 1.0.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m xgboost version 1.1.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m shap version 0.35.0 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from distutils.version import LooseVersion as Version\n",
    "import sys\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.7 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == min_ver:\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(sys.version)\n",
    "if pyversion >= \"3.7\":\n",
    "    print(OK, \"Python version is %s\" % sys.version)\n",
    "elif pyversion < \"3.7\":\n",
    "    print(FAIL, \"Python version 3.7 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % sys.version)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'numpy': \"1.18.5\", 'matplotlib': \"3.2.2\",'sklearn': \"0.23.1\", \n",
    "                'pandas': \"1.0.5\",'xgboost': \"1.1.1\", 'shap': \"0.35.0\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1a** (3 points)\n",
    "\n",
    "You will work with the diabetes dataset in Problem 1 and you will split the data and preprocess it to get ready for training an ML model. First, read in the dataset into a pandas dataframe using the tab delimited file linked at [this page](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grading suggestion:\n",
    "- 3 points if they read in the file correctly using the delimiter argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data in this cell\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write you answer in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1b** (6 points)\n",
    "\n",
    "Answer the following questions with 1-2 paragraphs.\n",
    "\n",
    "Q1: Is the dataset IID or not? Why?\n",
    "\n",
    "Q2: Please decide what fraction of points will be in each set and explain your decision in a paragraph or two.\n",
    "\n",
    "Q3: Please explain in a paragraph or two why it is important to fit the preprocessors on the training set only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grading suggestion:\n",
    "- 2 points if they correctly argue that the dataset is IID.\n",
    "- 2 points for a correct argument. 60-20-20 is best, I'd still be OK with 80-10-10 if they have a reasonable argument.\n",
    "- 2 points for a good Q3 explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1c** (11 points)\n",
    "\n",
    "Based on your answers above, please perform a basic split and create training, validation, and test sets.\n",
    "\n",
    "Now that you have three sets, you can preprocess the data. Please decide for each feature which preprocessor you will use (no need to write text). Fit those preprocessors on the training set, then transform the sets.\n",
    "\n",
    "We discussed in class that it is important to split the data using various different random states so you can determine at the  end of the ML pipeline how much uncertainty in the test score the random splitting causes. Please use 10 random states and split/preprocess the data 10 times. \n",
    "\n",
    "Please make sure that your code is reproducable. The best way to check that is to print out which points are in e.g., the training set and rerun the cell a couple of times. If the same points are in the same set after every rerun, your code is reproducable. \n",
    "\n",
    "A couple of suggestions how you could structure your code is available below. \n",
    "\n",
    "\n",
    "One option:\n",
    "```python\n",
    "\n",
    "random_states = [...,...,...] # list of 10 numbers\n",
    "\n",
    "for random_state in random_states:\n",
    "    # whenever you need to set the random state, use `random_state`\n",
    "    \n",
    "    # split the data\n",
    "    \n",
    "    # preprocess the data\n",
    "    \n",
    "    # print stuff out to make sure your code is reproducable\n",
    "    \n",
    "```\n",
    "\n",
    "Second option:\n",
    "```python\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    random_state = 42 * i # feel free to replace 42 with your magic number.\n",
    "                          # the only important thing is that random_state has a different value in each iteration.\n",
    "    \n",
    "    # split the data\n",
    "    \n",
    "    # preprocess the data\n",
    "    \n",
    "    # print stuff out to make sure your code is reproducable\n",
    "    \n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grading suggestion:\n",
    "\n",
    "- 3 points for correctly splitting with train_test_split and setting the random_state\n",
    "- 2 points for correctly using a one-hot encoder on the gender and either the standard scaler or the min-max scaler on the rest (deduct a point if they also preprocess the target variable. in regression, the target variable stays as is)\n",
    "- 3 points if the fit the preprocessors to the training set and then transform everything\n",
    "- 3 points for correctly looping through 10 random states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2** \n",
    "\n",
    "We work with the [hand postures dataset](https://archive.ics.uci.edu/ml/datasets/Motion+Capture+Hand+Postures) in problem 2. This dataset has group structure. 14 users performing 5 hand postures with markers attached to a left-handed glove were recorded. Two different ML questions can be asked using this dataset. We will explore how the splitting and preprocessing differs for both questions in 2a and 2b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2a** (10 points)\n",
    "\n",
    "How would you prepare the data if we wanted to know how well we can predict the hand postures of a new, previously unseen user? Write down your reasoning (the usual 1-2 paragraphs are fine). Split the dataset into training, validation, and test sets, preprocess the sets, and loop through 10 random states similar to 1b. As usual, check for reproducability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grading suggestion\n",
    "- 6 points if they do group-split based on user ID and use the 'class' (the hand gesture) as the target variable \n",
    "    - it's ok if they use something else than GroupShuffleSplit as long as they split based on user ID\n",
    "- 2 points for using the standard scaler on each 33 features and fitting on the train only\n",
    "- 2 points for looping through 10 random states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Add your explanation here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2b** (10 points)\n",
    "\n",
    "How would you prepare the data if we wanted to identify a user based on hand postures? Follow the same steps as in 2a (explain your reasoning, split, preprocess, loop through 10 random states, check reproducability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grading suggestion\n",
    "- 6 points if they split based on the user as the target variable \n",
    "    - the perfect solution would be to do a stratified split based on the combination of class and user ID columns\n",
    "    - it is however also good if they do a stratified split on the user ID. this is important because some users have a few hundred postures measured while other uses have almost 10k postures.\n",
    "    - a simple train_test_split is not OK.\n",
    "- 2 points for using the standard scaler on each 33 features and fitting on the train only\n",
    "- 2 points for saving each set into csv files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add you code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
