{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>User</th>\n",
       "      <th>X0</th>\n",
       "      <th>Y0</th>\n",
       "      <th>Z0</th>\n",
       "      <th>X1</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Z1</th>\n",
       "      <th>X2</th>\n",
       "      <th>Y2</th>\n",
       "      <th>...</th>\n",
       "      <th>Z8</th>\n",
       "      <th>X9</th>\n",
       "      <th>Y9</th>\n",
       "      <th>Z9</th>\n",
       "      <th>X10</th>\n",
       "      <th>Y10</th>\n",
       "      <th>Z10</th>\n",
       "      <th>X11</th>\n",
       "      <th>Y11</th>\n",
       "      <th>Z11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>54.263880</td>\n",
       "      <td>71.466776</td>\n",
       "      <td>-64.807709</td>\n",
       "      <td>76.895635</td>\n",
       "      <td>42.462500</td>\n",
       "      <td>-72.780545</td>\n",
       "      <td>36.621229</td>\n",
       "      <td>81.680557</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56.527558</td>\n",
       "      <td>72.266609</td>\n",
       "      <td>-61.935252</td>\n",
       "      <td>39.135978</td>\n",
       "      <td>82.538530</td>\n",
       "      <td>-49.596509</td>\n",
       "      <td>79.223743</td>\n",
       "      <td>43.254091</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.849928</td>\n",
       "      <td>72.469064</td>\n",
       "      <td>-62.562788</td>\n",
       "      <td>37.988804</td>\n",
       "      <td>82.631347</td>\n",
       "      <td>-50.606259</td>\n",
       "      <td>78.451526</td>\n",
       "      <td>43.567403</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.329647</td>\n",
       "      <td>71.707275</td>\n",
       "      <td>-63.688956</td>\n",
       "      <td>36.561863</td>\n",
       "      <td>81.868749</td>\n",
       "      <td>-52.752784</td>\n",
       "      <td>86.320630</td>\n",
       "      <td>68.214645</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.142401</td>\n",
       "      <td>71.435607</td>\n",
       "      <td>-64.177303</td>\n",
       "      <td>36.175818</td>\n",
       "      <td>81.556874</td>\n",
       "      <td>-53.475747</td>\n",
       "      <td>76.986143</td>\n",
       "      <td>42.426849</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  User         X0         Y0         Z0         X1         Y1  \\\n",
       "0      1     0  54.263880  71.466776 -64.807709  76.895635  42.462500   \n",
       "1      1     0  56.527558  72.266609 -61.935252  39.135978  82.538530   \n",
       "2      1     0  55.849928  72.469064 -62.562788  37.988804  82.631347   \n",
       "3      1     0  55.329647  71.707275 -63.688956  36.561863  81.868749   \n",
       "4      1     0  55.142401  71.435607 -64.177303  36.175818  81.556874   \n",
       "\n",
       "          Z1         X2         Y2  ...   Z8   X9   Y9   Z9  X10  Y10  Z10  \\\n",
       "0 -72.780545  36.621229  81.680557  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "1 -49.596509  79.223743  43.254091  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2 -50.606259  78.451526  43.567403  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "3 -52.752784  86.320630  68.214645  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "4 -53.475747  76.986143  42.426849  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   X11  Y11  Z11  \n",
       "0  NaN  NaN  NaN  \n",
       "1  NaN  NaN  NaN  \n",
       "2  NaN  NaN  NaN  \n",
       "3  NaN  NaN  NaN  \n",
       "4  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/Postures.csv', skiprows=[1])\n",
    "df.replace(\"?\", np.nan, inplace=True)    # replace ? as nan\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduced_feature_training_posture(X, y, group_feature, model, param_grid, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a reduced feature model for the hand gesture dataset\n",
    "    Return the unique feature patterns and the correponding models and accuracies\n",
    "    \"\"\"\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=random_state)\n",
    "    other_idx, test_idx = next(gss.split(X, y, User))\n",
    "    # extract X_other, X_test, y_other, y_test, User_other, User_test\n",
    "    X_other, y_other = X.iloc[other_idx], y.iloc[other_idx]\n",
    "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "    User_other = User.iloc[other_idx]\n",
    "    \n",
    "    ## find all unique patterns of missing value in test set\n",
    "    mask = X_test.isnull()\n",
    "    unique_rows = np.array(np.unique(mask, axis=0))\n",
    "    all_y_test_pred = pd.DataFrame()\n",
    "    \n",
    "    print('there are', len(unique_rows), 'unique missing value patterns.')\n",
    "    \n",
    "    reduced_model = {\"sub_cols\": [], \"model\": []}\n",
    "    # divide test sets into subgroups according to the unique patterns\n",
    "    for i, unique_rows_sub in enumerate(unique_rows):\n",
    "#         print ('working on unique pattern', i)\n",
    "        \n",
    "        ## choose the according reduced features for subgroups\n",
    "        # get feature column names in this particular pattern\n",
    "        sub_cols = X_test.columns[~unique_rows_sub]\n",
    "        reduced_model['sub_cols'].append(sub_cols)\n",
    "        # 1.get the feature columns according to this pattern\n",
    "        sub_X_other = X_other[sub_cols]\n",
    "        # 2.get take sub_rows and sub_cols for X_test on this pattern\n",
    "        sub_rows = np.sum(mask == unique_rows_sub, axis=1) == unique_rows_sub.shape[0]\n",
    "        sub_X_test = X_test.loc[sub_rows, sub_cols]\n",
    "        # 3.cut the rows in the sub_X_other that have any nans\n",
    "        sub_X_other = sub_X_other.dropna()\n",
    "        # 4.cut the sub_Y_other and sub_y_test accordingly\n",
    "        sub_y_other = y_other.loc[sub_X_other.index]\n",
    "        sub_y_test = y_test.loc[sub_X_test.index]\n",
    "        # 5.cut user for group kfold\n",
    "        sub_User_other = User_other.loc[sub_X_other.index]\n",
    "#         print(len(set(sub_y_test)))\n",
    "        if len(set(sub_y_test)) == 1:\n",
    "            return 1\n",
    "    return 0\n",
    "#         ## prepare pipe\n",
    "#         numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "#         preprocessor = ColumnTransformer(remainder=\"passthrough\",\n",
    "#                                          transformers=[(\"num\", numeric_transformer, sub_cols)])\n",
    "#         pipe = Pipeline(steps=[(\"preprocessor\", preprocessor),\n",
    "#                                (\"classifier\", model)])\n",
    "#         # GroupKFold, k = min(4, length of unique value)\n",
    "#         gkf = GroupKFold(n_splits=min(4, len(sub_User_other.unique())))\n",
    "#         cv_idx = list(gkf.split(sub_X_other, sub_y_other, sub_User_other))\n",
    "#         # init grid\n",
    "#         grid = GridSearchCV(pipe, param_grid=param_grid, cv=cv_idx, \n",
    "#                             scoring=make_scorer(accuracy_score), \n",
    "#                             iid=False, n_jobs=-1)\n",
    "#         le = LabelEncoder()\n",
    "#         # label encode y incase there are class missing\n",
    "#         sub_y_other = le.fit_transform(sub_y_other)\n",
    "#         ## start training\n",
    "#         b_time = time.time()\n",
    "#         grid.fit(sub_X_other, sub_y_other)\n",
    "#         e_time = time.time()\n",
    "#         print(f\"took {round(e_time - b_time, ndigits=2)}s\")\n",
    "#         # refit using the best hyperparameters\n",
    "#         pipe.set_params(**grid.best_params_)\n",
    "#         pipe.fit(sub_X_other, sub_y_other)\n",
    "#         reduced_model[\"model\"].append(pipe)\n",
    "\n",
    "#         ## predict y, transform back and save\n",
    "#         sub_y_test_pred = pipe.predict(sub_X_test)\n",
    "#         print(f\"accuracy: {accuracy_score(le.transform(sub_y_test), sub_y_test_pred)}\")\n",
    "#         sub_y_test_pred = pd.DataFrame(le.inverse_transform(sub_y_test_pred), \n",
    "#                                        columns=['sub_y_test_pred'], index=sub_y_test.index)\n",
    "#         all_y_test_pred = all_y_test_pred.append(sub_y_test_pred)\n",
    "        \n",
    "#     all_y_test_pred = all_y_test_pred.sort_index()\n",
    "#     y_test = y_test.sort_index()\n",
    "#     total_acc = accuracy_score(y_test, all_y_test_pred)\n",
    "#     return reduced_model, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 10 unique missing value patterns.\n",
      "there are 9 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 9 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 9 unique missing value patterns.\n",
      "there are 9 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 9 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 9 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 9 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "\n",
      "split with only 1 class occurred 16 out of 20\n"
     ]
    }
   ],
   "source": [
    "y = df['Class']\n",
    "User = df['User']\n",
    "X = df.iloc[:, 2:]\n",
    "param_grid_logi = {'classifier__C': 1/np.logspace(-3,5,3)}\n",
    "model_logi = LogisticRegression(penalty='l1', solver='saga', max_iter = 10000, multi_class='auto')\n",
    "count = 0\n",
    "for s in range(20):\n",
    "    count += reduced_feature_training_posture(X, y, User, model_logi, param_grid_logi, 42+s)\n",
    "print()\n",
    "print(f\"split with only 1 class occurred {count} out of 20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduced_feature_training_user(X, y, stratify_ref, model, param_grid, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a reduced feature model for the hand gesture dataset\n",
    "    Return the unique feature patterns and the correponding models and accuracies\n",
    "    \"\"\"\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state, stratify=stratify_ref)\n",
    "    stratify_kfold_ref = stratify_ref.iloc[X_other.index]\n",
    "    \n",
    "    ## find all unique patterns of missing value in test set\n",
    "    mask = X_test.isnull()\n",
    "    unique_rows = np.array(np.unique(mask, axis=0))\n",
    "    all_y_test_pred = pd.DataFrame()\n",
    "    \n",
    "    print('there are', len(unique_rows), 'unique missing value patterns.')\n",
    "    \n",
    "    reduced_model = {\"sub_cols\": [], \"model\": []}\n",
    "    # divide test sets into subgroups according to the unique patterns\n",
    "    for i, unique_rows_sub in enumerate(unique_rows):\n",
    "#         print ('working on unique pattern', i)\n",
    "        \n",
    "        ## choose the according reduced features for subgroups\n",
    "        # get feature column names in this particular pattern\n",
    "        sub_cols = X_test.columns[~unique_rows_sub]\n",
    "        reduced_model['sub_cols'].append(sub_cols)\n",
    "        # 1.get the feature columns according to this pattern\n",
    "        sub_X_other = X_other[sub_cols]\n",
    "        # 2.get take sub_rows and sub_cols for X_test on this pattern\n",
    "        sub_rows = np.sum(mask == unique_rows_sub, axis=1) == unique_rows_sub.shape[0]\n",
    "        sub_X_test = X_test.loc[sub_rows, sub_cols]\n",
    "        # 3.cut the rows in the sub_X_other that have any nans\n",
    "        sub_X_other = sub_X_other.dropna()\n",
    "        # 4.cut the sub_Y_other and sub_y_test accordingly\n",
    "        sub_y_other = y_other.loc[sub_X_other.index]\n",
    "        sub_y_test = y_test.loc[sub_X_test.index]\n",
    "        # 5.cut user for group kfold\n",
    "        sub_stratify_kfold_ref = stratify_kfold_ref.loc[sub_X_other.index]\n",
    "#         print(f\"number of class: {len(set(sub_y_test))}\")\n",
    "        if len(set(sub_y_test)) == 1:\n",
    "            return 1\n",
    "    return 0\n",
    "        \n",
    "        ## prepare pipe\n",
    "#         numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "#         preprocessor = ColumnTransformer(remainder=\"passthrough\",\n",
    "#                                          transformers=[(\"num\", numeric_transformer, sub_cols)])\n",
    "#         pipe = Pipeline(steps=[(\"preprocessor\", preprocessor),\n",
    "#                                (\"classifier\", model)])\n",
    "#         # GroupKFold, k = min(4, length of unique value)\n",
    "#         skf = StratifiedKFold(n_splits= min(4, len(sub_User_other.unique())))\n",
    "#         cv_idx = list(skf.split(sub_X_other, sub_y_other, sub_User_other))\n",
    "#         # init grid\n",
    "#         grid = GridSearchCV(pipe, param_grid=param_grid, cv=cv_idx, \n",
    "#                             scoring=make_scorer(accuracy_score), \n",
    "#                             iid=False, n_jobs=-1)\n",
    "#         le = LabelEncoder()\n",
    "#         # label encode y incase there are class missing\n",
    "#         sub_y_other = le.fit_transform(sub_y_other)\n",
    "#         ## start training\n",
    "#         b_time = time.time()\n",
    "#         grid.fit(sub_X_other, sub_y_other)\n",
    "#         e_time = time.time()\n",
    "#         print(f\"took {round(e_time - b_time, ndigits=2)}s\")\n",
    "#         # refit using the best hyperparameters\n",
    "#         pipe.set_params(**grid.best_params_)\n",
    "#         pipe.fit(sub_X_other, sub_y_other)\n",
    "#         reduced_model[\"model\"].append(pipe)\n",
    "\n",
    "#         ## predict y, transform back and save\n",
    "#         sub_y_test_pred = pipe.predict(sub_X_test)\n",
    "#         print(f\"accuracy: {accuracy_score(le.transform(sub_y_test), sub_y_test_pred)}\")\n",
    "#         sub_y_test_pred = pd.DataFrame(le.inverse_transform(sub_y_test_pred), \n",
    "#                                        columns=['sub_y_test_pred'], index=sub_y_test.index)\n",
    "#         all_y_test_pred = all_y_test_pred.append(sub_y_test_pred)\n",
    "        \n",
    "#     all_y_test_pred = all_y_test_pred.sort_index()\n",
    "#     y_test = y_test.sort_index()\n",
    "#     total_acc = accuracy_score(y_test, all_y_test_pred)\n",
    "#     return reduced_model, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "there are 10 unique missing value patterns.\n",
      "\n",
      "split with only 1 class occurred 7 out of 20\n"
     ]
    }
   ],
   "source": [
    "y = df['User']\n",
    "X = df.iloc[:, 2:]\n",
    "stratify_other_test_ref = df['Class'].astype(str) + \"&\" + df['User'].astype(str)\n",
    "param_grid_logi = {'classifier__C': 1/np.logspace(-3,5,3)}\n",
    "model_logi = LogisticRegression(penalty='l1', solver='saga', max_iter = 10000, multi_class='auto')\n",
    "count = 0\n",
    "for s in range(20):\n",
    "    count += reduced_feature_training_user(X, y, stratify_other_test_ref, model_logi, param_grid_logi, 42+s)\n",
    "print()\n",
    "print(f\"split with only 1 class occurred {count} out of 20\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
