{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set 9\n",
    "\n",
    "**Problem 0** (-2 points for every missing green OK sign. If you don't run the cell below, that's -14 points.)\n",
    "\n",
    "Make sure you are in the DATA1030 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.7.6 | packaged by conda-forge | (default, Jun  1 2020, 18:33:30) \n",
      "[Clang 9.0.1 ]\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 1.18.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.2.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 0.23.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 1.0.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m xgboost version 1.1.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m shap version 0.35.0 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from distutils.version import LooseVersion as Version\n",
    "import sys\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.7 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == min_ver:\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(sys.version)\n",
    "if pyversion >= \"3.7\":\n",
    "    print(OK, \"Python version is %s\" % sys.version)\n",
    "elif pyversion < \"3.7\":\n",
    "    print(FAIL, \"Python version 3.7 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % sys.version)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'numpy': \"1.18.5\", 'matplotlib': \"3.2.2\",'sklearn': \"0.23.1\", \n",
    "                'pandas': \"1.0.5\",'xgboost': \"1.1.1\", 'shap': \"0.35.0\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will work with the [hand postures dataset](https://archive.ics.uci.edu/ml/datasets/Motion+Capture+Hand+Postures) in this problem set. We already split and preprocessed the dataset in PS4 problem 2 and we will build on the solution. We provide our solution for PS4 problem 2a as a starting point. Keep the missing values in the dataset, do not drop points or columns, or impute values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1a** (10 points)\n",
    "\n",
    "We will predict the hand postures of a previously unseen user and we will use XGBoost in Problem 1.\n",
    "\n",
    "Study the target variable and choose an evaluation metric. Please explain why you chose the metric (1-2 paragraphs). (3 points)\n",
    "\n",
    "Train an XGBoost model on the splitted and preprocessed data. Use early stopping and tune the max_depth parameter. (5 points)\n",
    "\n",
    "Accumulate the training, validation, and test scores. (2 points)\n",
    "\n",
    "NOTE: I recommend you print out partial results to see how the training progresses. If it takes a long time to train the models, change `n_splits` in `GroupShuffleSplit` to 1 instead of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users in train: [ 2  5  7  8  9 11 14]\n",
      "users in CV: [ 1  4  6 13]\n",
      "users in test: [ 0 10 12]\n",
      "users in train: [ 1  4  6  8  9 13 14]\n",
      "users in CV: [ 2  5  7 11]\n",
      "users in test: [ 0 10 12]\n",
      "users in train: [ 1  2  4  5  6  7 11 13]\n",
      "users in CV: [ 8  9 14]\n",
      "users in test: [ 0 10 12]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GroupKFold, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "df = pd.read_csv('data/Postures.csv', skiprows=[1])\n",
    "df.replace(\"?\", np.nan, inplace=True)    # replace ? as nan\n",
    "df.head()\n",
    "\n",
    "## delete these lines\n",
    "import xgboost\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "max_depth = [1,3,10,30,100]\n",
    "param_grid = {\"learning_rate\": [0.03],\n",
    "              \"n_estimators\": [10000],\n",
    "              \"seed\": [0],\n",
    "              #\"reg_alpha\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "              #\"reg_lambda\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "              \"missing\": [np.nan], \n",
    "              \"colsample_bytree\": [0.9],              \n",
    "              \"subsample\": [0.66]}\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "test_scores = []\n",
    "max_depths = []\n",
    "##\n",
    "\n",
    "\n",
    "# extract X, y and User\n",
    "y = df['Class']\n",
    "User = df['User']\n",
    "X = df.iloc[:, 2:]\n",
    "# init GroupShuffleSplit as follows\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=42)\n",
    "for nth_split, (other_idx, test_idx) in enumerate(gss.split(X, y, User)):    # group by User\n",
    "    # extract X_other, X_test, y_other, y_test, User_other, User_test\n",
    "    X_other, y_other = X.iloc[other_idx], y.iloc[other_idx]\n",
    "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "    User_other = User.iloc[other_idx]\n",
    "    User_test = User.iloc[test_idx]\n",
    "    # init GroupKFold, 3 folds\n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "    # create a temporary dictionary for kfold and test data\n",
    "    random_split_tmp = {'kfold_train':[], 'kfold_cv':[], 'test': []}\n",
    "    for train_idx, cv_idx in gkf.split(X_other, y_other, User_other):    # group by User_other\n",
    "        # extract X_train, X_train, y_cv, y_cv, User_train, User_cv\n",
    "        X_train, X_cv = X_other.iloc[train_idx], X_other.iloc[cv_idx]\n",
    "        y_train, y_cv = y_other.iloc[train_idx], y_other.iloc[cv_idx]\n",
    "        User_train = User_other.iloc[train_idx]\n",
    "        User_cv = User_other.iloc[cv_idx]\n",
    "        # check if ant user exist in any of the two splits\n",
    "        if set(User_train.unique()) == set(User_cv.unique()) or\\\n",
    "           set(User_train.unique()) == set(User_test.unique()) or\\\n",
    "           set(User_cv.unique()) == set(User_test.unique()):\n",
    "            raise ValueError('User exists in two or more of the splits')\n",
    "        # init StandardScaler\n",
    "        ss = StandardScaler()\n",
    "        # fit_transform on train\n",
    "        X_train = ss.fit_transform(X_train)\n",
    "        # transform cv and test\n",
    "        X_cv = ss.transform(X_cv)\n",
    "        X_test = ss.transform(X_test)\n",
    "\n",
    "        print('users in train:',User_train.unique())\n",
    "        print('users in CV:',User_cv.unique())\n",
    "        print('users in test:',User_test.unique())\n",
    "        \n",
    "        for d in max_depth:\n",
    "            param_grid['max_depth'] = [d]\n",
    "            XGB = xgboost.XGBClassifier()\n",
    "            XGB.set_params(**ParameterGrid(param_grid)[0])\n",
    "\n",
    "            XGB.fit(X_train,y_train,early_stopping_rounds=50,eval_set=[(X_cv, y_cv)], verbose=False)\n",
    "            y_train_pred = XGB.predict(X_train)\n",
    "            y_cv_pred = XGB.predict(X_cv)\n",
    "            y_test_pred = XGB.predict(X_test)\n",
    "            \n",
    "            train_scores.append(accuracy_score(y_train,y_train_pred))\n",
    "            val_scores.append(accuracy_score(y_cv,y_cv_pred))\n",
    "            test_scores.append(accuracy_score(y_test,y_test_pred))\n",
    "            max_depths.append(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1b** (10 points)\n",
    "\n",
    "Create a figure which shows the mean and stdev of the train and validation scores as a function of the max_depth parameter. (5 points)\n",
    "\n",
    "Determine which max_depth parameter gives the best validation score. (2 points)\n",
    "\n",
    "Write a paragraph about the train and validation scores and any peculiarity that you see on the plot with respect to high bias and high variance. (2 points)\n",
    "\n",
    "Finally, calculate the mean and stdev of the test score based on the best max_depth value. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82177438 0.86395777 0.99307044 0.99833414 0.99821525]\n",
      "[0.61847343 0.64093185 0.68797338 0.69550197 0.69520936]\n",
      "[0.50163218 0.43953845 0.50789123 0.48135059 0.49850975]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "0.48135059184194845\n",
      "0.19182525229230873\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "mean_train = np.zeros(len(max_depth))\n",
    "std_train = np.zeros(len(max_depth))\n",
    "mean_val = np.zeros(len(max_depth))\n",
    "std_val = np.zeros(len(max_depth))\n",
    "mean_test = np.zeros(len(max_depth))\n",
    "std_test = np.zeros(len(max_depth))\n",
    "\n",
    "train_scores = np.array(train_scores)\n",
    "val_scores = np.array(val_scores)\n",
    "test_scores = np.array(test_scores)\n",
    "max_depths = np.array(max_depths)\n",
    "\n",
    "for d in range(len(max_depth)):\n",
    "    mean_train[d] = np.mean(train_scores[max_depths == max_depth[d]])\n",
    "    std_train[d] = np.std(train_scores[max_depths == max_depth[d]])\n",
    "    mean_val[d] = np.mean(val_scores[max_depths == max_depth[d]])\n",
    "    std_val[d] = np.std(val_scores[max_depths == max_depth[d]])\n",
    "    mean_test[d] = np.mean(test_scores[max_depths == max_depth[d]])\n",
    "    std_test[d] = np.std(test_scores[max_depths == max_depth[d]])\n",
    "\n",
    "print(mean_train)\n",
    "print(mean_val)\n",
    "print(mean_test)\n",
    "\n",
    "\n",
    "plt.errorbar(max_depth,mean_train,yerr=std_train,fmt='o',label='train')\n",
    "plt.errorbar(max_depth,mean_val,yerr=std_val,fmt='o',label='val')\n",
    "plt.semilogx()\n",
    "plt.ylabel('accuracy score')\n",
    "plt.xlabel('max_depth')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(max_depth[np.argmax(mean_val)])\n",
    "print(mean_test[np.argmax(mean_val)])\n",
    "print(std_test[np.argmax(mean_val)])\n",
    "\n",
    "# in my case, max_depth = 30 gives the best validation score and the corresponding mean test score is 0.48.\n",
    "# the plot shows high bias for low max_depth values, but we don't see overfitting. \n",
    "# There is no scenario where the trainins score is high but the validation score starts to decrease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2a** (10 points)\n",
    "\n",
    "You will start to implement the reduced features model. First, calculate what patterns exist in the test set and how many points belong to each pattern. (4 points)\n",
    "\n",
    "Then for each pattern, collect the corresponding subgroup from the training set. This subgroup would be used to train a classification model. (3 points)\n",
    "\n",
    "For each subgroup, check the target variable and print out the unique class labels and how many points belong to each class. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users in train: [ 2  5  7  8  9 11 14]\n",
      "users in CV: [ 1  4  6 13]\n",
      "users in test: [ 0 10 12]\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False] 25\n",
      "(array([2]), array([6]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True  True] 5201\n",
      "(array([1, 2, 5]), array([  48, 4824, 1471]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True  True  True  True  True  True] 1006\n",
      "(array([1, 2, 4, 5]), array([  48, 6740,   12, 5171]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True  True  True  True  True] 1451\n",
      "(array([1, 2, 3, 4, 5]), array([  48, 7400,   79, 1795, 6456]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 2207\n",
      "(array([1, 2, 3, 4, 5]), array([  49, 7476, 2349, 4099, 6623]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 4996\n",
      "(array([1, 2, 3, 4, 5]), array([ 701, 7502, 3902, 6583, 7027]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 4377\n",
      "(array([1, 2, 3, 4, 5]), array([3787, 7522, 6392, 6641, 7242]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 3318\n",
      "(array([1, 2, 3, 4, 5]), array([6319, 7536, 7724, 6661, 7280]))\n",
      "[False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 667\n",
      "(array([1, 2, 3, 4, 5]), array([6699, 7555, 7746, 6664, 7321]))\n",
      "[False False False False False False False False False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 238\n",
      "(array([1, 2, 3, 4, 5]), array([7103, 7555, 7748, 6664, 7327]))\n",
      "users in train: [ 1  4  6  8  9 13 14]\n",
      "users in CV: [ 2  5  7 11]\n",
      "users in test: [ 0 10 12]\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False] 25\n",
      "(array([2]), array([4]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True  True] 5201\n",
      "(array([1, 2, 5]), array([  48, 5095, 1726]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True  True  True  True  True  True] 1006\n",
      "(array([1, 2, 4, 5]), array([  48, 6877,   12, 4708]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True  True  True  True  True] 1451\n",
      "(array([1, 2, 3, 4, 5]), array([  48, 7324,   75, 2278, 5977]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 2207\n",
      "(array([1, 2, 3, 4, 5]), array([  48, 7368, 2338, 3683, 6473]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 4996\n",
      "(array([1, 2, 3, 4, 5]), array([1279, 7436, 3748, 6059, 6897]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 4377\n",
      "(array([1, 2, 3, 4, 5]), array([4103, 7564, 5233, 6117, 7112]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 3318\n",
      "(array([1, 2, 3, 4, 5]), array([6318, 7591, 7082, 6144, 7150]))\n",
      "[False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 667\n",
      "(array([1, 2, 3, 4, 5]), array([7159, 7600, 7896, 6146, 7191]))\n",
      "[False False False False False False False False False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 238\n",
      "(array([1, 2, 3, 4, 5]), array([7578, 7600, 7916, 6147, 7197]))\n",
      "users in train: [ 1  2  4  5  6  7 11 13]\n",
      "users in CV: [ 8  9 14]\n",
      "users in test: [ 0 10 12]\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False] 25\n",
      "(array([2]), array([2]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True  True] 5201\n",
      "(array([2, 5]), array([3507, 2333]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True  True  True  True  True  True] 1006\n",
      "(array([2, 5]), array([5591, 6263]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True  True  True  True  True] 1451\n",
      "(array([2, 3, 4, 5]), array([6530,    4,  503, 7243]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 2207\n",
      "(array([1, 2, 3, 4, 5]), array([   1, 6646,   21, 3308, 7624]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 4996\n",
      "(array([1, 2, 3, 4, 5]), array([ 600, 6738, 1782, 6824, 7644]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 4377\n",
      "(array([1, 2, 3, 4, 5]), array([4494, 6886, 3955, 6926, 7644]))\n",
      "[False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 3318\n",
      "(array([1, 2, 3, 4, 5]), array([7365, 6927, 6084, 6963, 7644]))\n",
      "[False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 667\n",
      "(array([1, 2, 3, 4, 5]), array([7864, 6955, 6910, 6964, 7644]))\n",
      "[False False False False False False False False False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True] 238\n",
      "(array([1, 2, 3, 4, 5]), array([7887, 6955, 6932, 6965, 7644]))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GroupKFold, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "df = pd.read_csv('data/Postures.csv', skiprows=[1])\n",
    "df.replace(\"?\", np.nan, inplace=True)    # replace ? as nan\n",
    "df.head()\n",
    "\n",
    "# extract X, y and User\n",
    "y = df['Class']\n",
    "User = df['User']\n",
    "X = df.iloc[:, 2:]\n",
    "# init GroupShuffleSplit as follows\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=42)\n",
    "for nth_split, (other_idx, test_idx) in enumerate(gss.split(X, y, User)):    # group by User\n",
    "    # extract X_other, X_test, y_other, y_test, User_other, User_test\n",
    "    X_other, y_other = X.iloc[other_idx], y.iloc[other_idx]\n",
    "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "    User_other = User.iloc[other_idx]\n",
    "    User_test = User.iloc[test_idx]\n",
    "    # init GroupKFold, 3 folds\n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "    # create a temporary dictionary for kfold and test data\n",
    "    random_split_tmp = {'kfold_train':[], 'kfold_cv':[], 'test': []}\n",
    "    for train_idx, cv_idx in gkf.split(X_other, y_other, User_other):    # group by User_other\n",
    "        # extract X_train, X_train, y_cv, y_cv, User_train, User_cv\n",
    "        X_train, X_cv = X_other.iloc[train_idx], X_other.iloc[cv_idx]\n",
    "        y_train, y_cv = y_other.iloc[train_idx], y_other.iloc[cv_idx]\n",
    "        User_train = User_other.iloc[train_idx]\n",
    "        User_cv = User_other.iloc[cv_idx]\n",
    "        # check if ant user exist in any of the two splits\n",
    "        if set(User_train.unique()) == set(User_cv.unique()) or\\\n",
    "           set(User_train.unique()) == set(User_test.unique()) or\\\n",
    "           set(User_cv.unique()) == set(User_test.unique()):\n",
    "            raise ValueError('User exists in two or more of the splits')\n",
    "        # init StandardScaler\n",
    "        ss = StandardScaler()\n",
    "        # fit_transform on train\n",
    "        X_train = ss.fit_transform(X_train)\n",
    "        # transform cv and test\n",
    "        X_cv = ss.transform(X_cv)\n",
    "        X_test = ss.transform(X_test)\n",
    "\n",
    "        print('users in train:',User_train.unique())\n",
    "        print('users in CV:',User_cv.unique())\n",
    "        print('users in test:',User_test.unique())\n",
    "        \n",
    "        # DELETE CODE UNDER THIS LINE\n",
    "        \n",
    "        df_test = pd.DataFrame(data=X_test,columns=X.columns)\n",
    "        df_train = pd.DataFrame(data=X_train,columns=X.columns)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        \n",
    "        mask = df_test.isnull()\n",
    "        unique_rows, counts = np.unique(mask, axis=0,return_counts=True)\n",
    "        for i in range(len(counts)):\n",
    "            print(unique_rows[i],counts[i])\n",
    "        \n",
    "            sub_X_train = pd.DataFrame()\n",
    "            sub_Y_train = pd.DataFrame()\n",
    "            # 1.cut the feature columns that have nans in the according sub_X_test\n",
    "            sub_X_train = df_train[df_train.columns[~unique_rows[i]]]\n",
    "            # 2.cut the rows in the sub_X_train and sub_X_CV that have any nans\n",
    "            sub_X_train = sub_X_train.dropna()\n",
    "            # 3.cut the sub_Y_train and sub_y_CV accordingly\n",
    "            sub_Y_train = y_train.iloc[sub_X_train.index]\n",
    "            \n",
    "            # unique classes and their counts in train\n",
    "            print(np.unique(sub_Y_train,return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2b** (5 points)\n",
    "\n",
    "The reduced feature model might not always be a good approach especially in classification. As you saw in 2a, some subgroups have only a small number of data points or contain less than 5 of the original classification labels. Answer the two questions below. There is no need to code.\n",
    "\n",
    "Q1: What can you do if all points in a training set subgroup belong to one class? (2 points)\n",
    "\n",
    "Q2: What issues will you encounter if one training set subgroup contains less than 5 but more than 1 of the original classification labels? If you want, implement the reduced features model to answer this question. You can also figure it out without coding though. (3 points)\n",
    "\n",
    "Hint 1: you cannot collect more data. That's often not possible or too expensive so you need to work with the data you are given.\n",
    "\n",
    "Hint 2: read through the LabelEncoder manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: if a pattern is so specific that all points in a training set subgroup belong to one class, you can just predict that class for the corresponding points in the test set.\n",
    "\n",
    "Q2: You'll need to use the LabelEncoder to because some class labels will be missing. When you accumulate the predictions, you'll need to use the inverse_transform of the encoder to get back the original 5 labels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
