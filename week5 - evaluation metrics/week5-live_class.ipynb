{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mud card and piazza questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How can I compare the different evaluation metrics if they result in different suggestions? Like a model may have high R^2, low MSE or MAE**\n",
    "    - you do not compare different metrics!\n",
    "    - you choose one metric, calculate that metric for different models, and compare the metric values of different models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **It would be great if you could give rough regions of what is considered good and bad values of each metric, like when you mentioned that 0.35 was pretty good for log-loss.**\n",
    "    - keep in mind that you use the evaluation metric to compare different models\n",
    "    - you need to know two things:\n",
    "        - is the model better if my score is larger?\n",
    "            - for accuracy, recall, precision, f score, yes\n",
    "            - logloss is better if it is smaller\n",
    "            - MSE, RMSE, MAE are better if they are smaller\n",
    "            - R2 is better the closer to 1 it is\n",
    "        - what's the evaluation metric if my model has no predictive power (chance level)?\n",
    "           - for accuracy: it is the balance of the most popolous class\n",
    "           - for MSE: it's the variance of the target variable\n",
    "           - for RSME: it is the standard deviation of the target variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Is there any kind of way to determine which metric to choose? (I know it said consult with a team and that it is based on the question we are asking), but once we have a question formulated is there some kind of guide to help decide or do we try all metrics?**\n",
    "- **I still don't fully understand how to choose an evaluation metric. If there isn't stakeholder input and there aren't ethical considerations, how are evaluation metrics selected? How can we compare them?**\n",
    "    - if the problem is imbalanced classification: accuracy won't work so try the f1 score or maybe logloss\n",
    "    - if you want to avoid false positives, try precision or f_beta with beta < 1.\n",
    "    - if you want to avoid false negatives, try recall or f_beta with beta > 1.\n",
    "    - if you want to make sure your predicted probabilities are as accurate as possible because you'll need to rank your points based on the predicted probabilities for an intervetion, use logloss.\n",
    "    - in regression, RMSE or R2 are used most often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How can we illustrate confidence levels for classification and regression?**\n",
    "    - ML algorithms do not return confidence levels for one datapoint. \n",
    "        - regression models return one predicted value\n",
    "        - classification models return one predicted probability or one most likely class\n",
    "    - you could train several ML models using different random states for splitting, model training, etc.\n",
    "        - then you'd have multiple predictions for the same point in your test set and you can use that to calculate e.g., mean and std predicted values\n",
    "    - [here](https://stanfordmlgroup.github.io/projects/ngboost/) is a brand new ML technique which returns probabilistic predictions\n",
    "        - I haven't read the paper yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How should we choose an evaluation metric if our data is not iid?**\n",
    "    - tough question\n",
    "    - iid vs. non-iid usually doesn't impact which metric to choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What are the benefits of using MSE vs. MAE/RMSE and how do you decide which metrics to focus on?**\n",
    "- **in what situations would MAE be more useful than MSE?**\n",
    "- **For regression models I'm wondering how one might decide between MSE, RMSE, MAE and R^2 for evaluating a model?**\n",
    "    - MSE is often used but its unit is not the same as the target variable's unit\n",
    "        - for example, if the target variable is in dollars, the unit of MSE is in dollar**2\n",
    "    - this is why RMSE is sometimes preferred over MSE\n",
    "    - MAE is not used very often in my experience but it is a good metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Could you please explain more in detail about regression metrics? Which metrics should I use in a specific circumstance?**\n",
    "    - regression metrics are much easier than clasisfication metrics, that's why we spent less time on them\n",
    "    - you normally just decide if you want to use one of MSE, RMSE, MAE or R2 instead\n",
    "        - R2 is nice when you want a normalized metric\n",
    "            - a negative R2 indicates a bad model\n",
    "            - 0 shows baseline performance\n",
    "            - R2 is 1 if the model is perfect\n",
    "        - MSE, RMSE, MAE are all fine if you want a metric with lower values indicating a better model\n",
    "            - I prefer RMSE because it has the same unit as the target variable, but the same is true for MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **I am confused mostly about the C matrix and how it works and why it is important. As showing in ROC plot, what does that means of its x-axis and y-axis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Considering you get an n x n confusion matrix when choosing an evaluation metric for classification, does this mean it's a better idea to reduce the number of unique classifiers? For instance, you have ~50 unique classifiers (e.g. US states) ¬†but can reduce that number to 4 (regions in the US). Is that good practice, in light of choosing evaluation metrics?**\n",
    "    - terminology mix up!\n",
    "    - a classifier is a trained classification model, I think you mean class or label\n",
    "    - you could also talk about the number of categories in a categorical feature but that's not a target variable\n",
    "    - sometimes you can reduce the number of classes, I wouldn't generally recommend to do so. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **I am interested in when precision-recall curve would be more useful than the ROC curve.**\n",
    "    - when the dataset is imbalanced\n",
    "    - if the dataset is imbalanced, you want to choose an evaluation metric that does not use TN from the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How do we interpret the F-score in a confusion matrix? And how do we interpret the value of the logloss?**\n",
    "    - the f score is calculated based on the recall and precision, the closer it is to 1, the better.\n",
    "    - logloss is better the closer it is to 0. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What's the mechanic behind the f_beta score? Is it just a defined indicator? Would you please give some more examples about beta choosing?**\n",
    "    - I don't know what a defined indicator is\n",
    "    - the f score is the weighted harmonic mean of precision and recall and the beta parameter tells you how much weight you give to precision vs. recall (see question above for choosing beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Is the critical probability that we use as a cutoff for predicting the class considered a hyper-parameter of the model? Is it something that we tune along with other model hyper-parameters in order to maximize the value of our metric?**\n",
    "- **Also for the predict part, in the examples we specified p_crit but in looking at some of the documentation it doesn't seem like you get to specify any p_crit value, is there any way you can use a different value if you'd want to for some reason (would you ever want to/should you ever)?**\n",
    "    - that's a tough question because the critical probability is not a hyper-parameter of any ML algorithm\n",
    "    - you can tune it but it requires additional coding\n",
    "    - I usually tune the critical probability out of desperation when my model is not predictive enough with the nominal 50% critical probability :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The .predict_proba and .predict methods come from the specific sklearn classifier but are methods that all the classifiers have. I am guessing that the predict methods all do the same thing using the probabilities but the underlying process for predict_proba would be different for each classifier?**\n",
    "    - that is exactly right!\n",
    "    - we will cover ML algorithms during the next two weeks so you'll see how different algorithms calculate probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The algorithm that provides every wrong answer is actually good right? You can just make conclusion opposite to what it predicts.**\n",
    "    - lol"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **I am stilled confused about quiz4, would you mind to go over it during class?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Would love to go over quiz 3! Confused what we classified as true positive, when it wasn't precisely binary (0,1,2 as options).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
