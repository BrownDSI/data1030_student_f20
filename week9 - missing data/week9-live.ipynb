{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mud card questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Why xgboost have the same performance with missiong value data and imputed data? Can you elaborate more on that?**\n",
    "    - it is dataset-dependent which method will perform best\n",
    "    - we only had missing values in 3 features of the house price dataset and not too many points either\n",
    "    - so all missing data techniques will performs similarly well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **which situation is most appropriate for XGBoost**\n",
    "    - XGB is just another ML algorithm like linear/logistic regression, SVMs, RFs, etc.\n",
    "    - it can be used on any dataset\n",
    "    - it is the default technique I use when my data contains missing values but that's only my subjective choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Secondly, in Quiz 1 I keep getting 677 and not the correct answer. I found the correct code and plugged it in right after training on the imputed dataset but this was my answer. Is there a catch here?**\n",
    "    - no catch, I'm not sure what happened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How would we interpret the results of an XGBoost model, e.g. to a non-data scientist?**\n",
    "    - the same way you interpret the results of an SVM, RF, KNN :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **It sounds like XGBoost is a more powerful tree-based algorithm than the Random Forest implementation in SKLearn. Is there ever a case when it is better to use Random Forest or should we just opt for XGBoost as a default?**\n",
    "    - you can use XGB as the default if you understand early stopping and the differences between RF and XGB\n",
    "    - RF might be somewhat less accurate on datasets but it is a simplier technique with less hyperparameters than XGB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **I had trouble with the XGBoost and how it determines the patterns through code?**\n",
    "- **I'm confused about the XGBoost. Not about any particular question about it, just about the entire thing.**\n",
    "    - XGB doesn't determine the patterns, you do.\n",
    "    - XGB is a tough method to use, I recommend you go through the blog post, paper, maybe look for a couple of kaggle kernels that use the technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **So when I looked on stackoverflow I too found the same code :**\n",
    "\n",
    "```python\n",
    "# clf is a XGBoost model fitted using the sklearn API\n",
    "dump_list = clf.get_booster().get_dump()\n",
    "num_trees = len(dump_list)\n",
    "```\n",
    "**I had loaded XGBoost properly but clf didnt come up, do you know why this would happen?**\n",
    "    - we called the model `XGB` and not `clf` in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **\"I always thought that the Iterative Imputer did exactly what the reduced features model was doing, so what exactly is the IterativeImputer doing? Is there another approach that does not involve building a model for every missing pattern?**\n",
    "    - The iterative imputer treats a feature with missing values as the target variable and uses the other features to try to predict the missing values\n",
    "    - The reduced features model doesn't impute the missing values. It creates a submodel using a subset of points with a subset of features that do not contain missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **When using the reduced-features model, how do we select which of the different models do we used, do we just apply it based on the missing data in the test set?**\n",
    "    - we don't select the models this way but the subset of the points to use, but yes.\n",
    "    - generally the test set is used \n",
    "    - once you deploy, you might need to train additional models if the unseen data contains missingness patterns not seen in the test set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **In Quiz2, we have 8 models to train with 3 continuous features. However, what if we only have the 3 continuous features? Does it make sense to train a model with 3 missing values?**\n",
    "    - careful there!\n",
    "    - you have a plethora of other features that are complete. all of those features will be used in all of the models\n",
    "    - if all three features contain missing values, you'll train a model using the other (complete) features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **For the reduced features model, we worked with a dataset with a limited number of points. Does that mean that when there are way more missing values in the continuous features (100s of columns with such missing values), we have to build patterns for the TF table with all those columns? Or could we pick and choose the columns for which we want the reduced features model to handle the missing values?**\n",
    "    - usually you wouldn't pick columns by hand but build one model for each pattern you find in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Can we discuss more on the imbalanced data, like the way we should deal with it? It is a reflective topic, as we usually face an imbalanced dataset in practical problems.**\n",
    "    - could you let me know what topic you'd like me to discuss that wasn't discussed in the last video of the module?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Is SMOTE ever the right thing to do then?**\n",
    "    - I stay away from it, but that's just my subjective opinion\n",
    "    - others like the method and apply it\n",
    "    - you can apply it too as long as you are aware of how it will impact your confusion matrix and potentially your metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **In an imbalanced dataset, what would be a lower bound on total data points to potentially achieve some predictive power?**\n",
    "    - if you split your data into three sets (train, val, test), you should have at least one point of the minority class in each set\n",
    "    - if 99% of points belong to a majority class, that means you should have 300 points\n",
    "    - so you need to know the balance first, than you can calculate some crude lower bound this way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Is there any notes you can share on the intuition we should have for gradient boosting? I generally understand the concept but would love a little more specific explanation of how it works in the context of classification trees.**\n",
    "    - [sklearn](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) and [wikipedia](https://en.wikipedia.org/wiki/Gradient_boosting) are good places to start\n",
    "    - another great resource is [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/), chapter 10 talks about gradient boosting, other chapters cover trees, random forests and bunch of other techniques we covered as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Finally, why doesn't KNeighborClassifier have a class_weight variable? I was confused in the last Quiz because I thought all of them had a class_weight. I can imagine KNN doing this by simply assuming that every point that is within k-nearest adds int(class_weight) to the final results. Or is this just a problem of the package writer not implementing it...\"**\n",
    "    - I'm not sure either why it doesn't have a class_weight parameter\n",
    "    - I found this [stackoverflow entry](https://stackoverflow.com/questions/37876280/knn-with-class-weights-in-sklearn) on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **You said we should try all approaches with different random state to see which gives the best model but by using different approaches would we have different dataset to train on since the missing value are dealt differently?**\n",
    "    - excellent question!\n",
    "    - you can split the data first, and then apply all methods \n",
    "    - then all techniques will use the exact same train, val, and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The KNeighborsClassifier has the weights parameter ('uniform' or 'distance'). Is the reason we don't consider this a class_weight parameter because this one has to do with the weight of points instead of the classes?**\n",
    "    - that's exactly right!\n",
    "    - the KNN regressor has a `weight` parameter too although there are no classes in regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Quiz 2 and 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
