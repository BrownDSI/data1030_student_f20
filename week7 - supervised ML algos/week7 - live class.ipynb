{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mud card questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How does kernel density relate to the SVR?**\n",
    "    - SVMs with radial basis functions use a similar technique to 'smooth' the points\n",
    "    - but SVMs can have other kernels too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How can I select kernel for SVM?**\n",
    "    - check the manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **For SVC: Is there a typical range of gamma values that we'd be able to predict for an unseen dataset or would we use trial and error?**\n",
    "    - trial and error\n",
    "    - we don't know in advance which technique and what parameters will give the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **It seems like gamma = 1 always gives the best result for the examples. Under what circumstances would a different gamma be better?**\n",
    "    - gamma = 1 might work best on that one specific dataset but there is no guarantee it will be best on a different dataset\n",
    "    - always tune your hyperparameters to optimize performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **I was surprised to learn that random forests are not ideal for large datasets. Are linear and logistic regression the only tools used for large datasets?**\n",
    "    - it's not that RF must not be used on large datasets\n",
    "    - you just need to anticipate longer training times as a con for this technique\n",
    "    - RFs can also be parallelized, so you can use multiple cores to speedup the calcuations (`n_jobs` parameter)\n",
    "    - Jut FYI, SVMs are much slower on large datasets than RFs and SVMs can't be run parallel, so that's the technique I would not use on large datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **For decision tree in regression: I understood the explanation in Quiz 2 for why a tree can't be arbitrarily deep since we can't split a node if it doesn't have at least two points - but I want to clarify, does that mean that even for a huge dataset with thousands of features there will eventually be some max_depth parameter that is too big for the tree? Because the quiz says that the max_depth parameter can be arbitrarily large, but how can that be if the actual depth of the tree is limited? Shouldn't the max_depth reach a maximum achievable point for every tree?**\n",
    "    - by default, the tree will be split as long as there is one point on each leaf, let's call that depth `d_tree`\n",
    "    - if the max_depth value is smaller than `d_tree`, the tree will not reach a depth of `d_tree`, it will be truncated before and there will be more than 1 points in each leaf.\n",
    "    - if max_depth is equal or larger than `d_tree`, that has no impact on the tree and no error message will be given.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Finally, for random forests, why do we assign equal weights to all decision trees, regardless of their accuracy? Would we achieve a better overall prediction if we weighted decision trees by performance within a random forest?**\n",
    "- **on the tree question I understand that in that case all of the trees have equal weight but could you build a tree that has differently weighted subtrees that works as an effective classifier?**\n",
    "    - yes, you might but sklearn's random forest is not implemented that way.\n",
    "    - you can go to sklearn's repository, check if someone else requested this feature in the past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **I'm wondering how random forest makes decisions about which variables to place at what level on the tree and how continuous features are split(for example: >50 years old left branch, <=50 years old right branch etc.)?¬† Is it actually at random and sklearn picks the trees that perform best or is it more involved?**\n",
    "    - not random, it's more involved :)\n",
    "    - Classification and Regression Trees, Breiman et al., 1984\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Is the decision tree always binary?**\n",
    "    - all tree implementations that I am aware of are binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **could you please go over a bit of how to tune the other parameters of RF like min_split_weights, and to what extent are they actually impactful on the results on both classification or regression.**\n",
    "    - same as any other parameter, select a couple of values, calculate train and validation scores, check which value optimizes the validation score\n",
    "    - no clue how impactful that parameter is. it's different for every dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Is there a way to control the random forest parameters to bias it toward certain features or is it the randomness the strength of having multiple trees up to a point?**\n",
    "    - no way to manually bias the trees and you don't want to do that anyway\n",
    "    - the optimization algorithms will automatically find the model that's best on the training set\n",
    "    - the hyperparameter tuning will find the model that's best on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Can you explain the y_pred part and what is the function of train, test, and validation so I know what to fit or what to run which command on?**\n",
    "    - I would really prefer if you understood what you do and why rather than just apply the commands blindly\n",
    "    - the commands you need to apply are not always the same\n",
    "    - generally you fit your model on the training set, and you apply .predict or .predict_proba on the validation and test sets\n",
    "    - you apply .predict and .predict_proba only once on the test set, at the very end of the ML pipeline after you tuned the hyperparameters and you found the best parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How do we know whether to choose the class 0 or class 1 probability when working with y_new in quiz 1**\n",
    "    - usually the condition is that if pred_proba > p_crit, the point belongs to class 1, otherwise to class 0.\n",
    "    - with that condition, pred_proba should be the class 1 probabilities\n",
    "    - if you change the condition, you might need to change which class probabilities are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **what does smooth prediction means?**\n",
    "    - let's check the figures in the lecture notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **When finding the right fit using either SVM or random forests how do you precisely determine the parameters that will minimize bias and variance?**\n",
    "    - select the parameters that give the best validation score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What does the big data structure Z from the linear and logistic regression represent?**\n",
    "    - I don't know what you are refering to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Throughout the lecture & notebook we've only see examples with two features. I would like to see examples with more features.**\n",
    "    - if the dataset had more than two features, the code would be exactly the same except for  the preprocessing step\n",
    "    - we use the adult and house price datasets which have more than two features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **In quiz 3, we write the tree by our common sense or by looking a little bit into our dataset, however, if we later apply random forest classifier by Sklearn package in real-world data, how can we understand the generation process of those trees?**\n",
    "    - the trees are generated to maximize performance on the training data\n",
    "    - you can print out and visualize each individual tree, see [here](https://scikit-learn.org/stable/modules/tree.html#tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **In Quiz 5, I used a different random state (1) and got 10 as the gamma that maximizes accuracy, instead of 1. The same thing happened when I ran Andras' code. Why is that? I thought the random state didn't matter, apart from reproducibility**\n",
    "    - different random states give slightly different results. \n",
    "    - this is why the random state needs to be fixed and you need to try different states.\n",
    "    - if you try different states, you'll see the uncertainty due to the inherent randomness in splitting and some ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Quiz 3, Quiz 4, Quiz 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
