{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mud card questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How does GridSearchCV work? What's the difference between GridSearchCV and PredefinedSplit?**\n",
    "    - it basically loops through all combinations of the parameter grid and trains and evaluates models\n",
    "    - PredefinedSplit is for data splitting\n",
    "    - please read through the manual of these two functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **This is all really useful example code, but I'm still a little confused by how the gridsearch methods work and what exactly they return. Is there a simple way to describe how these function operate?**\n",
    "    - I recommend you print out the results dataframe we created, read through the manual, and check out the examples on the sklearn page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Is it good to include all the hyperparamters in one GridSearchCV to find the best combination of the hyparameters? Or do we need more steps in finding the best model?**\n",
    "    - yes, I'd tune all hyper-parameters in one GridSearchCV\n",
    "    - the only exception might be the critical probability, you'd tune that after GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How do you know which hyperparameters you want to tune? Is it a case of trial and error? Also, is the only way to check a hyperparameters importance to run the model?**\n",
    "    - over time, you'll gain more experience and you'll have a better feeling of which parameters you should tune\n",
    "    - but for now, try as many parameters as you can and use the validation scores to measure which parameters impact the score the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Are there any particular advantages to predefining a train/test split in advance and then using GridSearchCV compared to just splitting the data using k-fold splitting and then applying GridSearchCV?**\n",
    "    - if your data is IID, k-fold is fine\n",
    "    - if your data is not IID and you need a non-standard way to split your data, predefined split might be better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How do we split time series data with group structure?**\n",
    "    - that's currently not supported by sklearn so you need to write your own custom splitter function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **I see in the hyper parameter tuning with folds examples you do both preprocessing and model training behind the curtain. Since we can also use train_test_split (predefined split) for GridSearchCV, would you recommend us to do so if I later find out the result are the same for both ways. (I haven't tested this yet.)**\n",
    "    - I recommend using ML pipelines to combine preprocessing with the actual ML algorithm\n",
    "    - this is powerful because you can directly use the raw input data and it makes your code shorter and clearer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **We've been using different random states while checking the evaluation metrics and it seems like they do have a big effect on the value of the metrics. When building our own ML pipelines, what range of random states does it make sense to try? It feels like it is likely that there will always be a random state that could make the performance better.**\n",
    "    - DO NOT OPTIMIZE ON THE RANDOM STATE! \n",
    "    - there is no guarantee that a lucky state that does well on the test set will also do well on new data you'll get during deployment\n",
    "    - it doesn't really matter what range you try, it can be 0-9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **I'm wondering why for each iteration through a random state, we find the model that performs the best and calculate an accuracy score for the test data? Won't we likely end up with models with different hyperparameters as our optimal model for a given random state. In this case we would be computing the accuracy score for our test data with several different models and these test accuracy scores could ultimately influence our decisions about which model is best overall. In that case our test data has influenced our model. Would it make more sense to save the test data till the very end after our final model has been selected, for example by finding commonalities in the hyperparameters that performed well on the validation data?**\n",
    "    - excellent question!\n",
    "    - KFold does roughly what you suggest. If you have 4 folds, the best hyperparameters are selected based on the average of 4 validation scores so it reduces the randomness in the hyperparameters\n",
    "    - however if you calculate your test score only once, you won't know the uncertainty of your test score due to randomly splitting your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **For databases where there are fewer data points like there were in this case, is it typical to divide by time? Is there a way to divide for cases where it is not time-dependent data in order to create more data points?**\n",
    "    - it's project-specific\n",
    "    - think about what makes sense for your particular project\n",
    "    - given that some seizures go on for minutes, it makes sense for me to split the seizures into shorter chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **This was a really interesting project to hear about. Since many such projects are probably open source, how can we go about finding them? (Online resources/websites)**\n",
    "    - look for journals that publish in your area of interest and read papers published in that journal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **In time-series data, will some models have limitations? If so, what models will we typically use in time-series?**\n",
    "    - classical ML (the techniques we use in this class) typically aren't the best for time series data\n",
    "    - you'll learn about LSTMs next term which is the current state of the art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Should we consider other scores besides an accuracy score when choosing the optimal hyperparameters? What if the accuracy scores are very similar but one parameter value has a much higher precision?**\n",
    "    - accuracy should only be used in classification if your dataset is not imbalanced\n",
    "    - you should choose the score before you choose the hyperparameters and stick with it. \n",
    "    - you don't use multiple scores to select hyperparameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Would you explain more about function of n_jobs?**\n",
    "    - it describes how many cores should be used to run the function (e.g., GridSearchCV or RandomForest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Outside of splitting, What common mistake should we be aware of that can cause our model evaluation to be incorrect and/or off its true value?**\n",
    "    - a couple of times I left my target variable in the feature matrix and the ML model gave perfect predictions :D \n",
    "    - use linear models and SVMs without standardizing all features\n",
    "    - colinear features don't work well with linear models\n",
    "    - impute values when it doesn't make sense\n",
    "    - use an incorrect metric (e.g., accuracy when your dataset is imbalanced)\n",
    "    - generally not understanding your data, your problem, or the models you use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Quiz 1 and 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
