{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mud card and piazza questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **I understood that alpha affects how many feature weights are reduced to 0, but can this effect be recognized in the regression graphs? The shape of the curves in both graphs seems arbitrary.**\n",
    "    - if by regression graph, you mean the feature weights as a function of alpha then yes, it is arbitrary a little bit. \n",
    "    - for any given alpha value, the graphs show the feature weights that minimize the cost function on the training set.\n",
    "    - if alpha is sufficiently densely spaced, the curves change more or less smoothly\n",
    "    - not much else can be extracted from those figures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Which circumstance is better to use Ridge regression and which circumstance is better for Lasso regression ?**\n",
    "- **How can I determine which regularization method would be better used? Could you provide some examples?**\n",
    "- **Can you expand upon when we use Lasso regularization v. Ridge regularization? Is it helpful to use both ever?**\n",
    "- **What is an example of when we would use ridge regularization instead of lasso? Thanks!**\n",
    "- **When is it appropriate to use lasso or ridge regression and are there any limitations to these techniques?**\n",
    "    - there is no general advice \n",
    "    - lasso turns some of the feature weights to 0 so it is nice if you want to do feature selection\n",
    "    - ridge has a nicer 'behavior' meaning that the feature weights are a smoothly varying function of alpha\n",
    "    - try both and see which one performs better on your dataset\n",
    "    - you'll learn about a third regularization method called elastic net in the problem set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **\"It's not completely clear to me why Lasso regularization is good for feature selection. Why does the fact that feature weights go to zero for alpha =0 mean it's good for feature selection? If all thetas are zero, doesn't it mean there are no more features?\"**\n",
    "    - if the weight of a feature is 0, it means that the feature does not contribute to the prediction\n",
    "    - if you do the hyperparameter tuning properly, not all feature weights will be 0 but only some. \n",
    "    - the features with non-zero weights are important, the features with 0 weights can be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Is it possible to have small coefficients and still have overfitting. In this case would adding the penalty term to our loss function still be able to mitigate this issue?**\n",
    "    - it is theoretically possible yes. \n",
    "    - and adding regularization would mitigate the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sometimes the regularization and different metrics do help improve the accuracy of the model. What other things can we use to tune the model?**\n",
    "    - one note: changing the evaluation metric will not improve the accuracy of the model. you just change how you measure the model's performance.\n",
    "    - here are a couple of things that might improve the performance of your model:\n",
    "        - you can generate more features\n",
    "        - sometimes it helps the model to exclude unimportant features (but do it on a data-driven way, not based on your intuition!)\n",
    "        - try different ML algorithms! I'll ask you to try at least three different ML algorithms in your final project and decide which one works best.\n",
    "        - try to tune more/different hyperparameters in your ML algorithm\n",
    "        - there is a method called stacking/blending where you combine the predictions of multiple ML models to create an even better model - we won't cover that in class but here is an excellent [blog post](https://mlwave.com/kaggle-ensembling-guide/) on the topic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Is the only way to check for overfit to plot the regression models? Are there ways to ensure that overfit does not occur from the outset?**\n",
    "- **In conceptual terms, how does the optimal alpha value provide the best balance between bias and variance? Still unsure about the bias-variance tradeoff in general.**\n",
    "    - overfitting: the model performs well on the training set that it was trained on but it performs poorly on the validation set. the model is too complex.\n",
    "    - underfitting: the model performs poorly on both the training and validation sets. the model is too simple.\n",
    "    - tuning the hyperparameters is the best way to ensure that your model doesn't underfit or overfit\n",
    "    - a model that performs best on the validation set doesn't overfit or underfit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How does the graph for a linear fit end up getting so spikey? I know we regularize that away anyway but wouldnt we at least expect it to look piecewise linear?**\n",
    "    - the model is linear with respect to every feature\n",
    "    - we generated multiple features (polinomials of the single original feature), that's why it became curvy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Can we go over quiz 5-6 again?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
