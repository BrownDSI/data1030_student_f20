{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mud card questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **I think SHAP was the muddiest part of the lecture.**\n",
    "- **Could we please go over again how to calculate SHAP values?**\n",
    "    - I highly recommend this online [book](https://christophm.github.io/interpretable-ml-book/)\n",
    "    - it has two chapters on shapely values and the shap package\n",
    "    - very well written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Could you speak a little more about how we can use dependence plots? It seems like a very informative way of displaying data but I could use some more intuition on the situations in which they're most useful.**\n",
    "    - dependence plots are excellent at visualizing non-linear correlations between two features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Since the force plot shows probability of one data point at a time, how many points should we look at? Will the conclusions from the force plot be different than the box plot showing influence of each feature?**\n",
    "- **\"Hope to know what kind of points we should look into when adopting SHAP or LIME method?**\n",
    "    - yes, the force plot is different for each data point\n",
    "    - the force plot is useful to spot-check some points and to make sure the predictions make sense\n",
    "        - check as many points as you can\n",
    "        - check points that are outliers in regression or with small and large predicted probabilities in classification\n",
    "    - the force plot is also useful in deployment when the model's prediction need to be explained to the person it is predicting for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Do shap and lime work the same way with regression models?**\n",
    "    - not really, no\n",
    "    - in regression, the shap values of a data point add up to the predicted value\n",
    "    - the LIME model develops local linear regression models in both regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **With the blue part of the formula we put the weight into consideration. Missing features will cause small |S| but what about the missing value in the features. Since when we interpret the model as our last step I think missing values are handled at this point. So when we interpret the model, both global and local feature importance metrics can be affected by how we handle the missing value and we the significance of one certain feature in practical might be different from what we seen from the plot.**\n",
    "    - yes, that's correct\n",
    "    - this is exactly why I recommend to always try multiple approaches and compare their performances\n",
    "    - you mostly care about optimizing your evaluation metric so that will tell you which approach you should choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **\"In linear/logistic regression feature importance, why do we add the final scaler at the end of the pipeline, rather than just applying a standard scaler to all features in the first place?\"**\n",
    "    - some features need to be one-hot encoded or ordinal encoded before you can apply a standard scaler on it\n",
    "    - if a feature initially contains strings, the standard scaler will throw an error message\n",
    "    - try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **When trying to interpret linear models does final_scaler = StandardScaler() only change our one-hot encoded features? And this is just so they all have the same mean and variance so comparisons can be made between coefficients?**\n",
    "    - features that were preprocess with the standard scaler will not change after the final scaler\n",
    "    - all other features (MinMax, OHE, and OE features) will change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What do the pluses and circles mean for the big red plus? Is it a classification situation where + is 1 and circles are 0?**\n",
    "    - yes, exactly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Did you generate that plot yourself? Can you provide the code for it? It's a very readable format.**\n",
    "    - it comes from the [LIME github repo](https://github.com/marcotcr/lime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LIME sounds very random. How it correct to train a linear regression model around the data point of interest when we don't know the underlying distribution or gradient? We could be throwing everything off by picking the wrong weighted points.**\n",
    "    - the points are generated randomly and they are weighted based on how close they are to the point of interest, so that's OK\n",
    "    - I think the idea is cool and it is based on the often used idea in physics that everything is linear if you look at it in small enough scales\n",
    "    - the thing I dislike about LIME is that it has a free parameter and it is not easy to select the best parameter value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Could you please explain \"weight\" and \"coverage\" again? I don't quite understand why the feature that lead to the split in the leaf node would be considered something important in XGBoost.**\n",
    "    - weight: the percentage representing the relative number of times a particular feature occurs in the trees of the model\n",
    "    - coverage: the relative number of observations related to this feature. For example, if you have 100 observations, 4 features and 3 trees, and suppose feature1 is used to decide the leaf node for 10, 5, and 2 observations in tree1, tree2 and tree3 respectively; then the metric will count cover for this feature as 10+5+2 = 17 observations. This will be calculated for all the 4 features and the cover will be 17 expressed as a percentage for all features' cover metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **I understand the difference between global and local feature importance, but I'm not sure I understand how to know which concept to apply in which situation.**\n",
    "    - ALWAYS!\n",
    "    - calculate global and local feature importances as part of you ML pipeline\n",
    "    - if you work with human data, local feature importances is a must\n",
    "    - even if you don't work with human data, understanding your model is a great benefit\n",
    "    - the predictions alone are not enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **You spoke about a con of permutation feature importance being no feature interactions, and a possible solution being permute two features to measure how important feature pairs are. But this is computationally expensive. How do we define \"too expensive\"? When is expensive worth it**\n",
    "    - that's a subjective decision and you need to make it\n",
    "    - is it worth to wait for the outcome for a day/week/month? you and your manager need to make that decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Quizzes and PS8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
